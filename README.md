# Simple Streamlit Chat with Memory
This project shows how to build a **chat interface with memory** using **Streamlit** and **Google's Gemini model** (gemini-2.0-flash).


## ðŸ“– Post
There is a full step-by-step tutorial on [Towards Data Science](https://towardsdatascience.com/step-by-step-guide-to-build-and-deploy-an-llm-powered-chat-with-memory-in-streamlit/) explaining this project in detail.

The post covers:

1. Create a new GitHub repository

2. Clone the repository locally

3. Set up a virtual environment

4. Create the project structure

5. Get your API Key

6. Store your API Key securely

7. Choose the model

8. Build the chat UI

9. Prompt engineering techniques

10. Set generate_content parameters

11. Display chat history

12. Implement chat with memory

13. Create a reset button

14. Deploy the app

15. Monitor API usage on Google Cloud Console



## ðŸš€ How to Run
1. Install the required libraries:

```
pip install -r requirements.txt
```

2. Set up your API Key:

Create a .env file with:

```
API_KEY=your_google_api_key_here
```


3. Run the app:

```
streamlit run app.py
```



## ðŸ“„ License
This project is open for educational purposes.
Feel free to reuse or adapt it!


Build with :heart: by Ale 

